# Human-Bench ğŸ¤–â†’ğŸ§‘

**A benchmark for human-like capability from virtual humans made of AI Agents.**

Human-Bench evaluates AI agents on realistic, multi-modal communication tasks that mirror how humans actually interact with personal assistantsâ€”across SMS, email, and voice calls, with context that spans days or weeks.

## Why Human-Bench?

Existing benchmarks test coding ability (SWE-bench), general reasoning (GAIA), or tool use (Ï„-bench). But none evaluate what makes a truly useful personal assistant:

- ğŸ“± **Multi-modal communication** (SMS, Email, Voice, Slack, Teams)
- ğŸ”— **Cross-channel context** ("About that email I sent..." during a phone call)
- âš¡ **Priority management** (handling interruptions gracefully)
- ğŸ“… **Long-term coherence** (planning a wedding over weeks)
- ğŸ¯ **Human-like interaction patterns** (not just Q&A)
- ğŸ”’ **Security & Privacy** (resisting unauthorized access attempts)
- ğŸ‘¥ **Team coordination** (managing multiple users and delegation)

## How It Works

**Human-Bench is a hosted benchmark service.** We handle all testing and evaluation.

1. **Register your AI assistant** at [humanbench.ai](https://humanbench.ai)
2. **Provide contact info**: Your persona's phone number, email, and name
3. **We run the tests**: We send SMS, emails, and phone calls to your assistant
4. **Get your score**: See detailed results and compare on the public leaderboard

No setup required - just register and test!

## Benchmark Structure

```
Level 1: Basic (Single turn, single channel)         â†’ Target: 95%+ success
Level 2: Multi-Turn (Conversations)                  â†’ Target: 85%+ success
Level 3: Real Work Tasks (Meetings, expenses, etc.)  â†’ Target: 75%+ success
Level 4: Cross-Modal (Context across channels)       â†’ Target: 70%+ success
Level 5: Priority Management (Interruptions)         â†’ Target: 60%+ success
Level 6: Long-Term Projects (Days/weeks)             â†’ Target: 40%+ success
Level 7: Adversarial (Security & Privacy)            â†’ Target: 98%+ success
Level 8: Team Coordination (Multi-user scenarios)    â†’ Target: 65%+ success
```

**Total: 500+ tasks** across 8 difficulty levels, testing real-world personal assistant capabilities.

**ğŸ”— Test Your Assistant**: [humanbench.ai](https://humanbench.ai)

## Task Format

Tasks are defined in YAML with clear success criteria:

```yaml
task_id: "SMS-001"
level: 1
category: "Basic Information Retrieval"
channel: "SMS"
difficulty: "easy"

input:
  from: "+15551234567"
  content: "What time is it in Tokyo?"

success_criteria:
  - type: "response_time"
    max_seconds: 30
  - type: "content_accuracy"
    requires: "correct timezone conversion"
  - type: "format"
    requires: "human-readable time format"

metadata:
  estimated_duration: "10s"
  tags: ["timezone", "information-retrieval"]
```

## Task Dataset

This repository contains the task definitions. Browse `tasks/` to see all benchmark scenarios.

Each task is carefully designed with:
- **Clear inputs**: Exact message content and context
- **Success criteria**: Objective evaluation rubrics
- **Human baselines**: Expected human performance
- **Complexity factors**: What makes each task challenging

## Results & Leaderboard

View live results and compare AI assistants at [humanbench.ai](https://humanbench.ai)

## Contributing

We welcome contributions! See [CONTRIBUTING.md](docs/contributing.md) for guidelines.

- ğŸ“ **Add tasks**: Propose new realistic scenarios
- ğŸ“Š **Improve criteria**: Better evaluation rubrics
- ğŸ› **Fix issues**: Report problems with existing tasks

## Citation

If you use Human-Bench in your research, please cite:

```bibtex
@misc{humanbench2025,
  title={Human-Bench: A Benchmark for Multi-Modal Personal AI Assistants},
  author={American Productivity Company},
  year={2025},
  url={https://github.com/american-productivity-company/human-bench}
}
```

## License

Human-Bench is released under the MIT License. See [LICENSE](LICENSE) for details.

---

**Built with â¤ï¸ by [American Productivity Company](https://righthand.ai)**
